---
title: "Hierarchical Clust"
author: "Ahilan Balakrishnan"
output: html_document
---

# What is Hierarchical Clustering?

-   Alternative to K-Means Clustering Algorithm (Has Pros and Cons)

-   Used to group together observations into the same group

    -   It will typically formulate the number of clusters (either top to bottom or bottom to top)

-   Does not require pre-specified number of clusters

-   Can Specify what distance metric to use

## Different types of Hierarchical Clustering

-   **Agglomerative Clustering (AGNES)**: Bottom up approach; Each observations starts as its own cluster and merges with other observations that are most similar. It will continue to do this until all the observations are in this one huge group.
-   **Divisive Hierarchical Clustering**: Top down approach; All observations are in one group. They start to split the more different (heterogeneous) they are. This keeps on iterating through until each observation is its own cluster.

## Different Forms of 'Distance'

-   Distance effect the shape of the cluster:

    1.  Euclidean Distance: It is the straight line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squares of the differences between corresponding coordinates

    2.  Manhattan Distance: It is also known as City block distance. It is the sum of the absolute differences of the coordinates of two points. It is calculated as the sum of the absolute differences of the x-coordinates and the y-coordinates.

    3.  Maximum Distance: It is also known as Chebyshev distance. It is the maximum absolute difference between any two corresponding coordinates of two points. It is calculated as the maximum of the absolute differences of the x-coordinates and the y-coordinates.

    4.  Cosine Distance: measures the cosine of the angle between two vectors in a high-dimensional space.

    5.  Mahalanobis Distance: takes into account the covariance structure of the variables

    6.  Jaccard Distance: used for binary data, it measures the similarity between two sets based on their overlap.

    7.  Bray-Curtis Dissimilarity: measures the dissimilarity between two samples based on the proportions of their features.

## Different Forms of Linking

-   **Complete Linkage Clustering**: Obtains the distance between **ALL** the observations in cluster 1 and cluster 2 and merges the clusters together if the distances are minimum.

-   **Average Linkage Clustering**: Computes distances between **ALL** the observations in cluster 1 and cluster 2 and averages the values.

-   **Centroid Linkage Clustering**: Calculates the centroid within each cluster and uses the distance between each centroid to determine to merge.

-   **Ward linkage :** Ward's linkage method is a hierarchical clustering algorithm that aims to minimize the variance of the clusters being merged at each step. In other words, it tries to find the most similar clusters to merge, where similarity is measured in terms of the sum of squares distances between the cluster centroids. By minimizing the variance of the merged clusters, the Ward's method seeks to form compact, spherical clusters.

```{r}

library(factoextra)
library(proxy)
dataset<-read.csv(file.choose())
dataset

# Overall goal is to do a hierarhical clustering observatiosn on the 4 features and we will esesentially compare the species to the type of clusters we have generated via the method. 

```

```{r}


dataset_new<-dataset%>% select('age','expenses')
df <- dataset_new
df <- na.omit(df) # remove any missing values 
df <- scale(df)   
head(df)
```

*One of the neat things about hierarchical clustering is that unlike k-means clustering where you pre-define the number of clusters, for this method, you do not have to identify this, esp if you're doing the top-down/bottom-up approach*.

```{r}
# Hierarchical Clustering Algorithm cosine and "Ward's method":

# He were also did the agglomerative/ bottom-up approach. 

# (complete, average, median, centroid..etc) Basically the type of method to utilize when you want to figure out how to merge the clusters with each other.

# Compute the distance matrix using cosine method
dist_mat <- dist(df, method = "cosine")

# Hierarchical clustering using the distance matrix using complete
hclust_res <- hclust(dist_mat, method ="ward.D2")

# Plot dendrogram
fviz_dend(hclust_res, k = 3, cex = 0.6, color_labels_by_k = TRUE)
```

```{r}
#checking count under each variable

final_clusters <- cutree(hclust_res, k=3)
table(final_clusters)
```

```{r}
# Hierarchical Clustering Algorithm using cosine and "complete" method

# Compute the distance matrix using cosine method
dist_mat <- dist(df, method = "cosine")

# Hierarchical clustering using the distance matrix using complete

hclust_complete <- hclust(dist_mat, method ="complete")

# Plot dendrogram
fviz_dend(hclust_complete, k = 3, cex = 0.6, color_labels_by_k = TRUE)
```

```{r}
#checking count under each variable

final_clusters <- cutree(hclust_complete, k=3)
table(final_clusters)
```

Essentially what this *dendodiagram*, we observe that each of our observations are being merged as they go up until everything is under one group.

Drawbacks to this approach is that if we have a billion observations then using this type of method isn't going to really cut it, then it that case we would probably use a k-means instead of hierarchical clustering.
